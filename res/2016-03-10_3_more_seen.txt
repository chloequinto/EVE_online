MONDAY, FEBRUARY 29TH - LEAP DAY – THE MOVE
Since we had opted away from SQL AlwaysON, we adjusted our timeline predictions and we were confident to make the 14:00 GMT mark.

Everyone was in and ready for action at 08:00 GMT.  

Following the final GO (as opposed to NO GO), the atmosphere was electric and huge excitement for the day to come. At 09:00 TQ went down for the last time in its old home.  

Then we progressed through the services needed, feeling good about our timeline as we approached the 10:30 marker. We started TQ for the first time ran a couple of checks and took TQ then back down.

At 11:00 we started a full stack EVE server, meaning it had all the auxiliary services like SSO, VGS, API, etc. followed by various QA tests from the development teams

Lots of small issues were discovered and fixed on the fly, like an emergency room where we had sudden influx of patients and the OPS team and EVE Developers just operated on them by severity and one by one we got to the point where every service was in a functional state.

The schedule was getting tight at this point, but it was clear that we did not want any quick workarounds. We wanted to do things right and if needed we would just extend the downtime.

Which we eventually had to do at 13:30, requesting an hour.

Shortly after that announcement things actually came together and at 14:20 GMT we started TQ in VIP with one last round of checks just to be 100% sure and then finally at 14:27 GMT we lifted VIP and it was a glorious moment when we saw the flood of players coming in…

..and then we got quickly pulled back to earth from our celebration when the Launcher showed almost an immediate stress signal and the web sites also went really high in utilization. After a while things calmed down and logins started flow correctly then the big bang came. Not the good kind.

BOOM TQ Database CPU went up to 100% without any warning or any indication that load was increasing, almost instant jump so what follows now is the tale of five startups: 


(click to enlarge)

1.       This is where we thought everything was fine and man we were happy

About 90 minutes into the run we hit the 100% CPU and it was diagnosed as a stored procedure which hits a bad query plan.
We had seen this happen on TQ before but it’s ultra-rare so we pinned this as a once in a blue moon, and such bad luck it hit us on this day. We went on and took down TQ and started up again.
2.       Second run we were scared it would hit us again, so we pulled back a little bit on the celebration.

Again the launcher and websites did not like the influx of players,  but as time passed that calmed down like before
Again roughly 90min into the run this same symptom hit us again though a different set of stored procedures. Another bad query plan taking down the server with up to date indexes that should not be happening
We raised a code red and called all hands on deck and EVE Development came up with a proposed fix to the stored procedure problem, but they agreed with the Database team that this just didn’t make any sense
We went on and took down the server applied the hot fix.
3.       Shortly after the third startup we saw that the market was not loading correctly and more errors were raised very shortly after the startup--mostly the result of us trying to get the server up and running to fast. Took it down for more preflight checks.

4.       Now we are somewhat into the night and we see social media both understandably raging and opposing positive posts which which boosted morale … Thank you ;)

This is where we still confident that the hotfix would get us through
Here we also found performance draining setting related to the launcher and websites and we also pushed more resources to the web front end machines as we did not have these type of “sudden influx” problems
We watched a countdown timer set to 90 min as that was our longest run. I can’t describe the feeling when we hit 90 minute mark … and it blew for the third time with 100% CPU on the Database.
Since the proposed fix and other measures we did were not working we shortly entertained the idea of fallback but opted first to failover the SQL cluster to a passive node and changed settings with RAM allocation
Had to Shut down yet again and things were becoming very unsettling to say the least.
5.       Fifth Time’s The Charm

We reset our 90 minute counter and  thanks to CCP Seagull who showed up shortly after mid-night with much more Red Bull and treats :)
The waiting game now was getting very intense and next steps were going to be hard if we had to go into a roll back scenario
At 23:47 we were ready for the fifth startup and lifted VIP
We split up the team and some went home to be ready day after others went to sleep in meeting rooms to take shifts throughout the night
Then we watched as the counter went to 80 min …. 85 …. 88 ..... 90……… 91 …………….….92 and then time passed and we hit our 2 hour goal and 2.5 hour goal and finally at 3 hours of run time we felt we were in the clear! 
TQ ran smoothly that night, and within Operations we had set a goal when this whole project started to go live on the 29th and since the fifth startup lifted VIP before midnight we still claim we hit our high level plan.

Sadly, promptly at 09:00 GMT March 1st TQ took itself down which was due to one of couple of automatic downtime settings which were forgotten to set back to its regular 11:00 GMT setting. Just plain and simple human error at fault.

This was how that eventful Monday played through and I can tell you that going through a project of this magnitude over such a long time is extremely hard but also extremely gratifying when you see the fruit of your collective labor.
