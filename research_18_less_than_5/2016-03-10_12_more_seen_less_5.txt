The EVE Online Database cluster
So during the extended downtime we took a huge step towards our upcoming data center move. 
One of the main issues we had to overcome was how to get the 2.5 TB database from one data center to the other with minimal downtime.  Also, in addition to the move we need to upgrade both the OS and SQL version (from Windows 2008 R2 to Windows 2012 R2 and SQL 2012 Enterprise to SQL 2014 Enterprise).
Of these two issues, upgrading to SQL 2014 is the biggest concern.  Or rather, not so much the upgrade, but the fallback plan in case we have to revert.  You see, once you upgrade a DB from an older version of SQL you cannot move that database back to an earlier version.  The only way is a DB restore from the same (or earlier) version.
So we decided that the least risk would be to upgrade the TQ DB servers to Win2012/SQL2014 while still in our existing DC.  This meant that on the big move day we had so much less to worry about and we have an extremely familiar environment to work within in-case something went wrong. 
In addition to this, we identified our preferred method of getting the DB to the new Datacenter – SQL AlwaysOn replication.  We could not do this before as the machines have to be in the same Windows Cluster/same OS versions.
So the plan was to merge two separate two-node SQL failover instances into one Windows Cluster and use AlwaysOn to keep the data in sync.  This would give us the ability to fail back from the new DC to the old one if we wanted (which we don’t, but we could!!)!
Part of what took us so long was that we added a pretty amazing (or so we think) test step into the mix…
We started by evicting the passive node of our production DB cluster, re-imaging it with latest OS, and joining it to our existing Windows Cluster in the new Datacenter to create a new single node SQL Cluster.  Lots of stuff involved here in getting a Windows Cluster to span two datacenters--much thanks to the SysAdmin and Network guys!!  
Just as a note, running with no failover partner isn’t something we like to do, but it was necessary and would be for less than 24 hours.
So at 09:00 GMT we shut down everything and continued!  We did things like move the SQL Cluster IP from the ‘old’ cluster to the new single node cluster and copied the smaller DB’s to the new disks.  For the large DB’s just remapped SAN volumes and reattached the databases.
But this brings us to the DB upgrade step. If we attached the SQL 2012 DB to our new SQL 2014 instance it would run an internal upgrade, at which point it would become incompatible with all previous SQL versions.  If something didn’t work for whatever reason, we would have had no choice but to do a restore of the backups we took earlier in the morning.  We would not be able to simply move the volumes back to the old cluster (which was still running, just as a single node as our fallback)
Enter brilliant idea – prior to presenting the SQL 2012 volumes to the new 2014 Cluster we would take a SAN snapshot of those volumes.  We give the snapshot volumes to the new cluster, attach the databases from the snapshot disks, and let the upgrade happen.  We started in VIP to check out some stuff. Things looked good so we removed those DB’s – unmapped the snapshots, destroyed the snapshots, and gave the cluster the ‘real’ disks and did it all again after a reboot.
It took time which contributed to missing the initial target time, but we felt it was worth it!
We added disks to the cluster, setup mount points, attached databases, started Tranquility in VIP mode again.  Woohoo, everything looked great so we opened the gates!
Shortly after opening, we went back to the old cluster and stole its now lonely node.  This was added back into the new cluster and we’re redundant again!
For the most part everything went smoothly… We did have several issues with bringing disks online and it meant a reboot or two (each reboot is 15 minutes).  This cost us the over-extension but we are very pleased with getting to this stage.